{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='mistral:7b-instruct-q5_K_M')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_together import Together\n",
    "\n",
    "#llm = Together(model=\"mistralai/Mistral-7B-Instruct-v0.2\", max_tokens=1000)\n",
    "llm = Ollama(model=\"mistral:7b-instruct-q5_K_M\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a chatbot that answers questions in a helpful and informative way.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a chatbot that answers questions in a helpful and informative way.\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! I am a chatbot designed to help answer any questions you may have in a helpful and informative manner. How can I assist you today?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"query\": \"Introduce yourself.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! I am a chatbot designed to assist you with any questions or information you may need. My goal is to provide helpful and accurate responses in a friendly and efficient manner. How can I assist you today?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = prompt | llm | output_parser\n",
    "chain2.invoke({\"query\": \"Introduce yourself.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an AI-powered chatbot designed to assist you with a wide range of information and tasks. I am capable of answering questions on various topics such as general knowledge, weather, news, sports, and more. I can also help you schedule appointments or set reminders for important events. Additionally, I am programmed to provide helpful tips and suggestions based on your needs and preferences. Please feel free to ask me anything and I will do my best to assist you!"
     ]
    }
   ],
   "source": [
    "for s in chain2.stream({\"query\": \"Introduce yourself and describe your capabilities.\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterator, List\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "from together import Complete\n",
    "\n",
    "class StreamingTogether(Together):\n",
    "    def _stream(self, prompt: str, stop: List[str] | None = None, run_manager: CallbackManagerForLLMRun | None = None, **kwargs: Any) -> Iterator[GenerationChunk]:\n",
    "        stream = Complete.create_streaming(prompt=prompt, model=self.model, stop=stop, **kwargs)\n",
    "        for chunk in stream:\n",
    "            if isinstance(chunk, str):\n",
    "                out = chunk\n",
    "            else:\n",
    "                out = chunk.generated_text or \"\"\n",
    "            yield GenerationChunk(text=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Hello! I'm an advanced AI designed to answer questions and provide information. I can help with a wide range of topics, from general knowledge and trivia, to more complex subjects like science, mathematics, and technology. I can also provide definitions, translations, and weather updates. I'm here to make your life easier and more convenient. Let me know how I can assist you today.\n",
      "---\n",
      "\n",
      "AI: Hello! I'm an advanced AI designed to answer questions and provide information. I can help with a wide range of topics including but not limited to: math problems, science questions, historical facts, language translations, and even provide explanations on complex concepts. I'm here to make your life easier and more informed. Let me know how I can assist you today.</s>"
     ]
    }
   ],
   "source": [
    "sllm = StreamingTogether(model=\"mistralai/Mistral-7B-Instruct-v0.2\", max_tokens=1000)\n",
    "s_chain = prompt | sllm | output_parser\n",
    "print(s_chain.invoke({\"query\": \"Introduce yourself and describe your capabilities.\"}))\n",
    "print(\"---\")\n",
    "for s in s_chain.stream({\"query\": \"Introduce yourself and describe your capabilities.\"}):\n",
    "    print(s, end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
